# üöÄ ***Configura√ß√£o de Data Lake com HDFS para trabalhar com alta disponibilidade***

## üìñ **Descri√ß√£o do Projeto:**

Este projeto configura um Data Lake utilizando HDFS (Hadoop Distributed File System) para garantir alta disponibilidade e resili√™ncia. A configura√ß√£o envolve a instala√ß√£o e ajuste do Hadoop, Zookeeper, e a cria√ß√£o de um cluster com alta disponibilidade. O objetivo √© criar um ambiente robusto capaz de lidar com grandes volumes de dados e garantir a continuidade dos servi√ßos em caso de falhas.

## **Principais Funcionalidades:**

- **Alta Disponibilidade:** Configura√ß√£o de um cluster Hadoop com m√∫ltiplos Namenodes e Datanodes para garantir a continuidade dos servi√ßos.
- **Replica√ß√£o de Dados:** Utiliza√ß√£o do Zookeeper para gerenciar a replica√ß√£o e o failover autom√°tico entre os nodes.
- **Seguran√ßa e Resili√™ncia:** Implementa√ß√£o de chaves SSH para acesso sem senha entre os nodes e configura√ß√£o de failover autom√°tico para garantir a integridade dos dados.


## üõ†Ô∏è **Ferramentas Utilizadas:**

- **Hadoop:** Sistema de processamento distribu√≠do que permite o armazenamento e an√°lise de grandes volumes de dados.


## üìã **Descri√ß√£o do Processo**
* Fazer o download do JDK, Hadoop e Zookeeper;
* Editar as vari√°veis de ambientes dos 3 arquivos;
* Fazer o clone da primeira m√°quina para as demais, pois as configura√ß√µes at√© aqui ser√£o as mesmas;
* Editar vari√°veis de ambiente .Bashrc;
* Criar chaves SSH para conex√£o entre m√°quinas do cluster Hadoop sem senha;
* Configurar os arquivos do Hadoop;
* Inicializar o cluster de alta disponibilidade.

## üíª **Comandos:** 

### Criar VM com Red Hat
---
### Verificar lista usu√°rios priorit√°rios: 

sudo vi /etc/sudoers >  adicionar o usu√°rio "aluno" na lista junto com o root
---
### Download JDK/Hadoop/ZOOKEEPER

Fazer o Download JDK 8, descompactar e mover para a pasta: sudo mv jdk1.8 /opt/jdk

Fazer o Download HADOOP, descompactar e mover para a pasta: sudo mv hadoop /opt/hadoop

Fazer o Download Zookeeper, descompactar e mover para a pasta: sudo mv zookeeper /opt/zookeeper
---
### Ir para a pasta home (~) e editar o arquivo .bash_profile com as vari√°veis de ambiente:
```
#Java JDK 1.8
export JAVA_HOME=/opt/jdk
export JRE_HOME=/opt/jdk/jre
export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin

#Hadoop
export HADOOP_HOME=/opt/hadoop
export PATH=$PATH:$HADOOP_HOME/bin

#Zookeeper
export ZOOKEEPER_HOME=opt/zookeeper
export PATH=$PATH:$ZOOKEEPER_HOME/bin
```
---
### Clone do namenode / Edi√ß√£o de configura√ß√µes das VMs

#### Criar clone do Namenode para o Secondary e o datanode

Obs: Como estamos clonando, √© importante fazer o refresh do MAC adress das duas m√°quinas clonadas. Al√©m disso, nas configura√ß√µes da VM a rede precisa estar no modo "Bridge adapter"

#### Alterar o nome das m√°quinas:  

sudo vi /etc/hostname (nn1.dsa.com / nn2.dsa.com / dn1.dsa.com)

hostnamectl set-hostname dn1.dsa.com / nn2.dsa.com

#### Ajustar o arquivo de rede: 

sudo vi /etc/hosts com o endere√ßo e o hostname de cada m√°quina

Testar com o comando "ping"
---
### Variaveis para arquivo .bashrc

#Incluir as linhas abaixo no arquivo ~/.bashrc em todos os nodes do cluster
```
export HADOOP_HOME=/opt/hadoop
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export JAVA_HOME=/opt/jdk
export ZOOKEEPER_HOME=/opt/zookeeper
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin
```
---
### Configurandos as chaves ssh sem senha

ssh-keygen -t rsa

Ap√≥s esse comando √© s√≥ dar enter nas configura√ß√µes, pois queremos a conex√£o sem senha

Precisamos copiar a chaver p√∫blica para o diret√≥rio authorized_keys em todos os nodes, portanto execute os comando abaixo no namenode:

#### Para o namenode2:

cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

ssh-copy-id -i .ssh/id_rsa.pub aluno@nn2.dsa.com

#### Para o datanode:

cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

ssh-copy-id -i .ssh/id_rsa.pub aluno@dn1.dsa.com


#Testando a conectividade do ssh:

ssh aluno@nn2.dsa.com

ssh aluno@dn1.dsa.com

---

### Configurando o namenode

Acesse /opt/hadoop/etc/hadoop

#### vi core-site.xml
```
<property>
 <name>fs.defaultFS</name>
 <value>hdfs://ha-cluster</value>
</property>

<property>
 <name>dfs.journalnode.edits.dir</name>
 <value>/home/hadoop/HA/data/jn</value>
</property>

 <property>
  <name>ha.zookeeper.quorum</name>
  <value>nn1.dsa.com:2181,nn2.dsa.com:2181,dn1.dsa.com:2181</value>
 </property>
```

#### vi hdfs-site.xml
```
<property>
 <name>dfs.namenode.name.dir</name>
 <value>/home/hadoop/HA/data/namenode</value>
 </property>
 <property>
 <name>dfs.replication</name>
 <value>1</value>
 </property>
 <property>
 <name>dfs.permissions</name>
 <value>false</value>
 </property>
 <property>
 <name>dfs.nameservices</name>
 <value>ha-cluster</value>
 </property>
 <property>
 <name>dfs.ha.namenodes.ha-cluster</name>
 <value>nn1,nn2</value>
 </property>
 <property>
 <name>dfs.namenode.rpc-address.ha-cluster.nn1</name>
 <value>nn1.dsa.com:9000</value>
 </property>
 <property>
 <name>dfs.namenode.rpc-address.ha-cluster.nn2</name>
 <value>nn2.dsa.com:9000</value>
 </property>
 <property>
 <name>dfs.namenode.http-address.ha-cluster.nn1</name>
 <value>nn1.dsa.com:50070</value>
 </property>
 <property>
 <name>dfs.namenode.http-address.ha-cluster.nn2</name>
 <value>nn2.dsa.com:50070</value>
 </property>
 <property>
 <name>dfs.namenode.shared.edits.dir</name>
 <value>qjournal://nn1.dsa.com:8485;nn2.dsa.com:8485;dn1.dsa.com:8485/ha-cluster</value>
 </property>
 <property>
 <name>dfs.client.failover.proxy.provider.ha-cluster</name>
 <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
 </property>
 <property>
 <name>dfs.ha.automatic-failover.enabled</name>
 <value>true</value>
 </property>
 <property>
 <name>dfs.ha.fencing.methods</name>
 <value>sshfence</value>
 </property>
 <property>
 <name>dfs.ha.fencing.ssh.private-key-files</name>
 <value>/home/hadoop/.ssh/id_rsa</value>
 </property>
```
---
### Configurando o zookeeper

Acesse: /opt/zookeeper/conf

Fa√ßa a c√≥pia do diret√≥rio `zoo_smple.cfg` com outro nome: cp 

zoo_smple.cfg zoo.cfg

Edite o `zoo.cfg`:
```
dataDir=/home/hadoop/HA/data/zookeeper
Server.1=nn1.dsa.com:2888:3888
Server.2=nn2.dsa.com:2888:3888
Server.3=dn1.dsa.com:2888:3888
```
---
### Criar diret√≥rios

#Criar a pasta /home/hadoop/HA/data/jn em todas as m√°quinas que foram configuradas o Jornalnode (nesse caso as 3 m√°quinas)

#Criar a pasta /home/hadoop/HA/data/namenode nos dois namenodes para grava√ß√£o de metadados do namenode (namenode e 
secondary namenode)

#Criar a pasta /home/hadoop/HA/data/datanode no datanode para grava√ß√£o de metadados do datanode (Apenas no datanode)

#Adicionar o par√¢metro abaixo no arquuivo hdfs-site.xml para o datanode:
 ```
    <property>
 <name>dfs.datanode.name.dir</name>
 <value>home/hadoop/HA/data/datanode</value>
 </property>
```
---
### Configura√ß√£o do arquivo workers

#Arquivo Worker

Em todas as m√°quinas do cluster, editar o arquivo workers e adicionar a lista de datanodes: 

1. Editar o arquivo: /opt/hadoop/etc/hadoop/workers 

2. Remover a op√ß√£o de localhost 

3. Incluir a lista de datanodes, um em cada linha (deve ser usado o mesmo nome configurado no arquivo /etc/hosts
---
### Configura√ß√£o do Zookeeper

#Configura√ß√£o da ordem no zookeeper
Crie o diret√≥rio /home/hadoop/HA/data/zookeeper

Crie e edite o aquivo myid com a indenta√ß√£o da m√°quina dentro do cluster (Namenode = 1, secondary = 2 e datanode = 3)
Deve ser feito para todas as m√°quinas

---

### Inicializa√ß√£o do Cluster HA

OBS: Antes de iniciar o cluster √© importante limpar todos os diret√≥rios de metadados 

Para o Journalnode precisamos configurar o jdk nas m√°quinas que rodar√£o esse daemon, portanto acesse: cd /opt/hadoop/etc/hadoop e edite o arquivo `hadoop-env.sh`:
export JAVA_HOME=/opt/jdk

1. **Inicializa√ß√£o do Journal Node nas 3 m√°quinas do cluster**

hdfs --daemon start journalnode

2. **Format do Namenode 1 (somente na primeira inicializa√ß√£o do cluster)**

hdfs namenode -format

3. **Verifica se o firewall est√° ativo**

sudo firewall-cmd --state

Firewall precisa estar desativado em todas as m√°qunas:

sudo firewall-cmd --state

sudo systemctl stop firewalld

sudo systemctl disable firewalld

4. **Inicializa√ß√£o do NameNode no NameNode Ativo**

hdfs --daemon start namenode

5. **Copiar os metadados do NomeNode Ativo para o StandBy (apenas na primeira inicializa√ß√£o, executar no NameNode Standby)**

hdfs namenode -bootstrapStandby

6. **Inicializa√ß√£o do NameNode no NameNode Standby**

hdfs --daemon start namenode

7. **Inicializa√ß√£o do Zookeeper em todas as m√°quinas do cluster**

zkServer.sh start

8. **Inicializa√ß√£o do DataNode**

hdfs --daemon start datanode

9. **Formatar o HA State (apenas na primeira inicializa√ß√£o)**

hdfs zkfc -formatZK

10. **Inicializa o Zookeeper HA Failover Controller (nos dois NameNodes)**

hdfs --daemon start zkfc

11. **Checar se os NameNodes est√£o configurados em HA**

hdfs haadmin -getServiceState nn1

Acesse  o endere√ßo ip para verificar o estado do cluster HDFS:
Endere√ßo IP da m√°quina:50070 no navegador web

---
## Contato

Se tiver d√∫vidas ou sugest√µes sobre o projeto, entre em contato comigo:

- üíº [LinkedIn](https://www.linkedin.com/in/henrique-k-32967a2b5/)
- üê± [GitHub](https://github.com/henriquekurata?tab=overview&from=2024-09-01&to=2024-09-01)
